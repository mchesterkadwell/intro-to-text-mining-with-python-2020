{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Exploring Text\n",
    "\n",
    "---\n",
    "---\n",
    "Before we can analyse text we have to clean and prepare it.\n",
    "\n",
    "## Tokenising the Text\n",
    "_Tokenising_ means splitting a text into meaningful elements, such as words, sentences, or symbols.\n",
    "\n",
    "To do this we are going to use [spaCy](https://spacy.io), a free and open-source Natural Language Processing (NLP) package. If you're interested in learning how to work with spaCy more broadly for a variety of NLP tasks I recommend the tutorial [Natural Language Processing with spaCy in Python](https://realpython.com/natural-language-processing-spacy-python/).\n",
    "\n",
    "---\n",
    "#### spaCy and NLTK\n",
    "[NLTK](https://www.nltk.org/) was the first open-source Python library for Natural Language Processing (NLP), originally released in 2001, and it is still a valuable tool for teaching and research. Much of the literature uses NLTK code in its examples, and researchers publish models, code and data sets for obscure languages designed for use with NLTK.\n",
    "\n",
    "If you are interested in using NLTK for the tasks presented in these notebooks instead of spaCy, please see last year's notebooks: https://github.com/mchesterkadwell/intro-to-text-mining-with-python\n",
    "\n",
    "NLTK has been overtaken in efficiency and ease of use by other more modern libraries, such as [SpaCy](https://spacy.io/), which uses machine learning. It is designed to use less computer memory and split workloads across multiple processor cores (or even computers) so that it can handle very large corpora easily. It also has excellent documentation. However, out of the box, spaCy only has support for a limited set of modern languages. It is necessary to train your own models to use spaCy for your use case.\n",
    "\n",
    "I am using spaCy in these notebooks to cut down on the amount of code I am presenting. The idea is to reduce the complexity so that beginners can concentrate on first principles.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data from a File\n",
    "\n",
    "Firstly, we need to reload into memory the file that was saved at the end of the notebook [3-choosing-and-collecting-text](3-choosing-and-collecting-text.ipynb).\n",
    "\n",
    "---\n",
    "### Opening and Reading Text Files\n",
    "We don't have enough time in this course to cover opening, reading and closing text files. Fortunately, it is not necessary to understand the next block of code to understand the rest of the notebook.\n",
    "\n",
    "If you would like to learn more about this by yourself, try this guide [Reading and Writing Files in Python](https://realpython.com/read-write-files-python/).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a module that helps with filepaths\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a filepath for the file\n",
    "text_file = Path('data', 'PREPPED-2199-0.txt')\n",
    "\n",
    "# Open the file, read it and store the text with the name `iliad`\n",
    "with open(text_file) as file:\n",
    "    iliad = file.read()\n",
    "\n",
    "iliad[0:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenising with spaCy\n",
    "\n",
    "First we import and load an English language model provided by spaCy (`en_core_web_sm`) and give it the name `nlp`, ready to do the work on the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we pass the text as an argument to the function `nlp` and spaCy does the rest. spaCy processes the text into a _document_ that contains a lot of information about the text.\n",
    "\n",
    "NB: This may take a while as the book is long. Watch until the `In [*]:` to the left of the code has finished and turned into an output number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = nlp(iliad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print out the document text stored in `document.text` just to check that spaCy has correctly parsed it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document.text[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document can be treated like a list of word tokens (and information about those tokens), which we can print out using a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [word.text for word in document]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here to get just the first 20 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy has split the text into sentences for us too. The document stores these sentences in `document.sents` and we can also print them out using a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sent.text for sent in document.sents]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of problems with the word tokens: the capitalisation of the words has been preserved, and some of the tokens have unwanted special characters or comprise single items of punctuation.\n",
    "\n",
    "## Normalising to Lowercase\n",
    "Normalising all words to lowercase ensures that the same word in different cases can be recognised as the same word, e.g. we want 'Shield', 'shield' and 'SHIELD' to be recognised as the same word.\n",
    "\n",
    "However, whether you choose to do this depends on the nature of your corpus and the questions you are investigating. For example, in another case, you may not want the word 'Conservative' to be conflated with the word 'conservative'.\n",
    "\n",
    "How can we lowercase all the tokens in the list of tokens `tokens`? By using the string method `lower()` and a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_lower = [token.lower() for token in tokens]\n",
    "tokens_lower[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Puctuation\n",
    "Punctuation such as commas, fullstops and apostrophes can complicate processing a corpus. For example, if punctuation is left in, the words \"poet\" and \"poet,\" might be considered to be different words.\n",
    "\n",
    "This is a complicated matter, however, and what you choose to do would vary depending on the nature of your corpus and what questions you wish to ask. It may be appropriate to remove punctuation at different stages of processing. In our case we are going to remove it *after* the text has been tokenised.\n",
    "\n",
    "We will replace *all* punctuation with the empty string `''`. (You do not need to understand this code fully.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a module that helps with string processing\n",
    "import string\n",
    "\n",
    "# Make a table that translates all punctuation to an empty value (`None`)\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "punc_table = {chr(key):value for (key, value) in table.items()}\n",
    "punc_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_nopunct = [token.translate(table) for token in tokens_lower]\n",
    "tokens_nopunct[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Non-Word Tokens\n",
    "\n",
    "We are still left with some problematic tokens that are not useful words, such as empty tokens (`''`) and newline characters (`\\r`, `\\n`).\n",
    "\n",
    "We can try a filter condition for the empty tokens. The operator `!=` is the negative equality operator, so `if token != ''` means \"if token is _not_ equal to the empty string\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_notempty = [token for token in tokens_nopunct if token != '']\n",
    "tokens_notempty[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operator `==` is the equality operator. If you just want a list of empty tokens, write the list comprehension replacing the `!=` with `==`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here to get a list of empty tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can remove all the newline characters by adding a condition that filters out all non-alphabetic characters. The string method to use is `isalpha()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token for token in tokens_notempty if token.isalpha()]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Data into a File\n",
    "\n",
    "Now we need to save the clean tokens into a file (`CLEAN-6130-8.txt`) and place it under the `data` folder. This is the reverse process to loading the data from a file that we did at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a module that helps with filepaths\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a filepath for the file\n",
    "tokens_file = Path('data', 'CLEAN-2199-0.txt')\n",
    "\n",
    "# Open a file and save the list of tokens inside it\n",
    "with open(tokens_file, 'w') as file:\n",
    "    file.writelines(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should inspect the file now to see what it looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook we have: \n",
    "\n",
    "* **Loaded** text from a file.\n",
    "* **Tokenised** the text into words and sentences with **spaCy**.\n",
    "* **Normalised** the words into **lowercase** and **removed non-word** tokens (punctuation and empty tokens)\n",
    "* **Saved** the clean tokens to file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
